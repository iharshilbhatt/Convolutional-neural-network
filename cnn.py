# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n9A7UuFcLM9Z4fWbeIqRHM9Kh1MZany9
"""

#import the basic liberaries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#import the dataset
import keras
from keras.datasets import fashion_mnist

#download the dataset
(train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()

plt.imshow(train_x[0])

train_y[0]

train_x.shape

test_x.shape

#preprocessing part of the dataset
train_x=train_x.reshape(-1,28,28,1)
test_x=test_x.reshape(-1,28,28,1)

test_x.shape

train_x.shape

# 0-255 --> 0-1. -   if we want this range of pixels we need to divie all the values by 255
#but we get all the values in integer so we get 0 fior 0-254 qand 1 for 255
# so we need to convert all the pixels value from integer to float and then divide by 255
train_x=train_x.astype('float32')
test_x=test_x.astype('float32')
train_x=train_x/255
test_x=test_x/255

train_y[0]

# one-hot encoding
from tensorflow.keras.utils import to_categorical
train_y_cat=to_categorical(train_y)
test_y_cat=to_categorical(test_y)

train_y_cat[0]

#model training
from sklearn.model_selection import train_test_split
train_x,valid_x,train_y,valid_y=train_test_split(train_x,train_y_cat,test_size=0.20)

#loading the liberaries for CNN model
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.layers import Dense,Activation,Flatten,Conv2D,MaxPooling2D,LeakyReLU
import tensorflow as tf

#1st Hidden layer
model=tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(32,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))

#2nd Hidden Layer
model.add(tf.keras.layers.Conv2D(64,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(tf.keras.layers.Conv2D(128,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128))
model.add(LeakyReLU(0.1))
#classification problem with 10 calsses --> softmax activation function
model.add(tf.keras.layers.Dense(10,activation='softmax'))

# compile the model
model.compile(loss=tf.keras.losses.categorical_crossentropy,optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])

model_train=model.fit(train_x,train_y,batch_size=64,epochs=20,verbose=1,validation_data=(valid_x,valid_y))

model_train.history

#plot the accuracy and loss in the graphical format
accuracy=model_train.history['accuracy']
val_accuracy=model_train.history['val_accuracy']
loss=model_train.history['loss']
val_loss=model_train.history['val_loss']
epochs=range(len(accuracy))

#plotting the accuracy graph
plt.plot(epochs,accuracy,label='training_accuracy')
plt.plot(epochs,val_accuracy,label='validation_accuracy')
plt.title("accuracy vs epochs")
plt.legend()
plt.show()

#plotting the loss graph
plt.plot(epochs,loss,label='training_loss')
plt.plot(epochs,val_loss,label='validation_loss')
plt.title("loss vs epochs")
plt.legend()
plt.show()

#1st Hidden layer
model=tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(32,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(tf.keras.layers.Dropout(0.25))

#2nd Hidden Layer
model.add(tf.keras.layers.Conv2D(64,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(tf.keras.layers.Dropout(0.25))

#3rd Hidden Layer
model.add(tf.keras.layers.Conv2D(128,(3,3),activation="linear",padding="same"))
model.add(LeakyReLU(0.1))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(tf.keras.layers.Dropout(0.25))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128))
model.add(LeakyReLU(0.1))
model.add(tf.keras.layers.Dropout(0.25))
#classification problem with 10 calsses --> softmax activation function
model.add(tf.keras.layers.Dense(10,activation='softmax'))

# compile the model
model.compile(loss=tf.keras.losses.categorical_crossentropy,optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])

from re import VERBOSE
model_train=model.fit(train_x,train_y,batch_size=64,epochs=20,verbose=1,validation_data=(valid_x,valid_y))

model_train.history

#plot the accuracy and loss in the graphical format
accuracy=model_train.history['accuracy']
val_accuracy=model_train.history['val_accuracy']
loss=model_train.history['loss']
val_loss=model_train.history['val_loss']
epochs=range(len(accuracy))

#plotting the accuracy graph
plt.plot(epochs,accuracy,label='training_accuracy')
plt.plot(epochs,val_accuracy,label='validation_accuracy')
plt.title("accuracy vs epochs")
plt.legend()
plt.show()

#plotting the loss graph
plt.plot(epochs,loss,label='training_loss')
plt.plot(epochs,val_loss,label='validation_loss')
plt.title("loss vs epochs")
plt.legend()
plt.show()